{"cells":[{"metadata":{"collapsed":true,"nbpresent":{"id":"b1f19b73-6823-4ad0-8cb2-0432ec0fa508"},"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Retail Sales Prediction"},{"metadata":{"nbpresent":{"id":"b5946145-61b4-42e0-a8f1-ebef17a22da9"},"slideshow":{"slide_type":"skip"},"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem Statement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Objective</a></span></li></ul></li><li><span><a href=\"#Data-description\" data-toc-modified-id=\"Data-description-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data description</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stores\" data-toc-modified-id=\"Stores-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Stores</a></span></li><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Features</a></span></li><li><span><a href=\"#Sales\" data-toc-modified-id=\"Sales-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Sales</a></span></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-and-Reading-the-datasets\" data-toc-modified-id=\"Importing-and-Reading-the-datasets-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Importing and Reading the datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initial-thoughts\" data-toc-modified-id=\"Initial-thoughts-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Initial thoughts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Steps-to-merge-the-datasets\" data-toc-modified-id=\"Steps-to-merge-the-datasets-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>Steps to merge the datasets</a></span></li><li><span><a href=\"#Final-dataset\" data-toc-modified-id=\"Final-dataset-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>Final dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Save-to-file\" data-toc-modified-id=\"Save-to-file-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Save to file</a></span><ul class=\"toc-item\"><li><span><a href=\"#CSV\" data-toc-modified-id=\"CSV-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>CSV</a></span></li><li><span><a href=\"#Pickle\" data-toc-modified-id=\"Pickle-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Pickle</a></span></li></ul></li><li><span><a href=\"#Data-description\" data-toc-modified-id=\"Data-description-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Data description</a></span></li><li><span><a href=\"#Univariate-Analysis\" data-toc-modified-id=\"Univariate-Analysis-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Univariate Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inferences-by-features\" data-toc-modified-id=\"Inferences-by-features-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Inferences by features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Temperature\" data-toc-modified-id=\"Temperature-3.4.1.1\"><span class=\"toc-item-num\">3.4.1.1&nbsp;&nbsp;</span>Temperature</a></span></li><li><span><a href=\"#Fuel-Price\" data-toc-modified-id=\"Fuel-Price-3.4.1.2\"><span class=\"toc-item-num\">3.4.1.2&nbsp;&nbsp;</span>Fuel Price</a></span></li><li><span><a href=\"#Markdown1-to-MArkdown-5\" data-toc-modified-id=\"Markdown1-to-MArkdown-5-3.4.1.3\"><span class=\"toc-item-num\">3.4.1.3&nbsp;&nbsp;</span>Markdown1 to MArkdown 5</a></span></li><li><span><a href=\"#CPI\" data-toc-modified-id=\"CPI-3.4.1.4\"><span class=\"toc-item-num\">3.4.1.4&nbsp;&nbsp;</span>CPI</a></span></li><li><span><a href=\"#unemployment\" data-toc-modified-id=\"unemployment-3.4.1.5\"><span class=\"toc-item-num\">3.4.1.5&nbsp;&nbsp;</span>unemployment</a></span></li></ul></li></ul></li><li><span><a href=\"#Pair-Plots\" data-toc-modified-id=\"Pair-Plots-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Pair Plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notable-Inferences\" data-toc-modified-id=\"Notable-Inferences-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Notable Inferences</a></span></li></ul></li><li><span><a href=\"#Correlation-Heatmap\" data-toc-modified-id=\"Correlation-Heatmap-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Correlation Heatmap</a></span></li><li><span><a href=\"#EDA-Final-words\" data-toc-modified-id=\"EDA-Final-words-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>EDA Final words</a></span></li><li><span><a href=\"#Missing-Values\" data-toc-modified-id=\"Missing-Values-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Missing Values</a></span><ul class=\"toc-item\"><li><span><a href=\"#KNN-Imputation\" data-toc-modified-id=\"KNN-Imputation-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>KNN Imputation</a></span></li><li><span><a href=\"#Delete-Markdowns-from-dataset\" data-toc-modified-id=\"Delete-Markdowns-from-dataset-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span>Delete Markdowns from dataset</a></span></li></ul></li><li><span><a href=\"#Other-data-cleaning-techniques\" data-toc-modified-id=\"Other-data-cleaning-techniques-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Other data cleaning techniques</a></span></li></ul></li><li><span><a href=\"#Headsup-for-next-steps\" data-toc-modified-id=\"Headsup-for-next-steps-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Headsup for next steps</a></span></li><li><span><a href=\"#Preparing-Test-dataset\" data-toc-modified-id=\"Preparing-Test-dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Preparing Test dataset</a></span></li><li><span><a href=\"#Model-1---No-data-transformation\" data-toc-modified-id=\"Model-1---No-data-transformation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model 1 - No data transformation</a></span></li></ul></div>"},{"metadata":{"collapsed":true,"nbpresent":{"id":"e109f388-76aa-4f10-8f63-9ce120093ffd"},"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Problem Statement\n\nThis project is to predict the sales across different stores. \nThe data containing historical sales data for 45 stores located in different regions - each store contains a\nnumber of departments. The company also runs several promotional markdown events throughout the\nyear. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor\nDay, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in\nthe evaluation than non-holiday weeks. Within the Excel Sheet, there are 3 Tabs – Stores, Features and\nSales"},{"metadata":{"nbpresent":{"id":"c52fcedc-f32c-446f-af38-bf5705888930"},"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"### Objective\n\nThe objective of this project is to \n\n1. Predict the department-wide sales for each store for the following year\n\n2. Model the effects of markdowns on holiday weeks\n\n3. Provide recommended actions based on the insights drawn, with prioritization placed on largest business\nimpact"},{"metadata":{"nbpresent":{"id":"a58230cd-497e-47c0-a909-75b97d2c191c"},"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Data description\n\nThere are 3 datasets used in this project. Their respecitve details as follows:\n\n### Stores\n\nAnonymized information about the 45 stores, indicating the type and size of store\n\nStore - Store ID\n\nType - Type of Store (A,B,C)\n\nSize – Size of the store\n\n### Features\n\nContains additional data related to the store, department, and regional activity for the given dates.\n\nStore – Store ID\n\nDate – Week start date\n\nTemperature - average temperature in the region\n\nFuel_Price - cost of fuel in the region\n\nMarkDown1-5 - anonymized data related to promotional markdowns. MarkDown data is only available\nafter Nov 2011, and is not available for all stores all the time. Any missing value is marked with NA\n\nCPI - the consumer price index\n\nUnemployment - the unemployment rate\n\nIsHoliday - whether the week is a special holiday week\n\n### Sales\n\nHistorical sales data, which covers to 2010-02-05 to 2012-11-01. Within this tab you will find the\nfollowing fields:\n\nStore – Store ID\n\nDept – Department ID\n\nDate – Week start date\n\nWeekly_Sales -sales for the given department in the given store\n\nIsHoliday - whether the week is a special holiday week\n\nThe dataset contains weekly sales data over the period of 3 years. Although data is clean for many features, there are mission values for Markdown columns anbd no data description about what these columns are. There could be challenges in understanding these columns and accommodating them to fit into the model along with handling lot of missing values there columns poccess"},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"### Importing and Reading the datasets\n\nThe data is located in 3 csv files \n\nSales - Retail Sales Prediction-Sales data-set.csv\n\nFeatures - Retail Sales Prediction-Features data set.csv\n\nStores - Retail Sales Prediction-Stores data-set.csv\n\nOverall steps for importing and reading the datasets are:\n\n1. import pandas - very useful for the data analysis\n2. Create a dictionary which holds the data read from all datasets - Dictionary is used to hold the data as a single object. This can be very convinient instead of creatig multiple variables\n3. For each csv file, use pandas read.csv method by inputting the relevant file name\n\nAll the data read will be stored as 3 different dataframes within a single dictionary\n\n"},{"metadata":{"hide_input":false,"nbpresent":{"id":"717f30fd-42c5-469d-9d07-acb26c7cbee8"},"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"hide_input":false,"nbpresent":{"id":"f1407ebf-389c-41a1-a508-922b3b626f7a"},"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"# Store all the csv as python dataframes. Storing in single variable for convenience\ndata={\n    \"sales\": pd.read_csv('../input/retaildataset/sales data-set.csv'),\n    \"features\": pd.read_csv('../input/retaildataset/Features data set.csv'),\n    \"stores\": pd.read_csv('../input/retaildataset/stores data-set.csv')\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(data['sales'].head())\nprint(len(data['sales']))","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"a7ed16e3-dab8-48df-a54d-019b4384fec9"},"trusted":true},"cell_type":"code","source":"\nprint(data['stores'].head())\nprint(len(data['stores']))","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"1c108fce-c04e-400c-b95d-0e90e879da23"},"trusted":true},"cell_type":"code","source":"\nprint(data['features'].head())\nprint(len(data['features']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Initial thoughts\n\nUpon inspecting the data from 3 datasets, its evident that \n\n1. Sales data contains historical data\n2. Stores contain the information regarding stores. This dataset doesn't add value on its own but combining store information with sales information helps in understanding the releation between sales across different type of stores\n3. Features dataset contains sales information with aditional features. \n\nUpon carefully obeserving Features and Sales, its evident that there are few variables in Sales and Features in common and features has few addititional variables. Combining all 3 datasets helps in having a simplified single dataset and it helps leveraging more value out of the data."},{"metadata":{},"cell_type":"markdown","source":"##### Steps to merge the datasets\n\n1. Join any 2 datasets based on the common columns. This can be similar to Left join in SQL\n2. With the dataframe resulting from step 1, join the 3rd dataframe following similar approach as Step 1"},{"metadata":{"code_folding":[],"nbpresent":{"id":"fec2fd2e-9b3d-4c39-8b17-c063e65d8143"},"trusted":true},"cell_type":"code","source":"# this df stores the merged df og features and stores csv\n#data['merged'] = pd.merge(data['features'],data['stores'], on='Store',how='left')","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"nbpresent":{"id":"3b1705ce-07cc-4bf6-9125-7a3a33c5ce08"},"trusted":true},"cell_type":"code","source":"# This df contains all 3 df joined int one\n#data['merged_total'] = pd.merge(data['sales'], data['merged'],  how='left', left_on=['Store','Date','IsHoliday'], right_on = ['Store','Date','IsHoliday'])","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"31d2282f-a335-463b-a69a-5c1fbc5982ad"},"trusted":true},"cell_type":"code","source":"#raw_data = data['merged_total']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Final dataset\n\nAs a result of merging all 3 dataframes, final dataset has a total of 421570 observations and 16 columns"},{"metadata":{"nbpresent":{"id":"f5c8b64e-97fb-4672-bb3b-a066af62d5e2"},"trusted":true},"cell_type":"code","source":"#raw_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save to file\n\nSave the merged dataframe to a csv file and pickle for future reference."},{"metadata":{},"cell_type":"markdown","source":"#### CSV\n\nFile can be saved as csv using pandas to_csv method. "},{"metadata":{"nbpresent":{"id":"5a40355e-45bd-4d80-9e4d-af7161de2957"},"trusted":true},"cell_type":"code","source":"#raw_data.to_csv('Retail_Sales_Merged.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Pickle\n\nPickle is python's way to store the dataframes as python objects. Saving any dataframe to pickle helps in resuing the pickle object directly to read the dataframe to session\n\nThis file is stored on the disk and can be exported to any file storage sevice thus giving the flexibility resuing the object to read directly from storage locations when there is change in working environments.\n\nAlso, pickling helps to avoid all the data reading steps from csv and all steps involved in merging dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = pd.read_pickle('../input/capstone/Retail_Sales_Merged.pkl')\nraw_data['Date'] = pd.to_datetime(raw_data['Date'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### Data description"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nmatplotlib.use('nbagg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of all numeric columns, Store, Dept and Size seem categorical variables. "},{"metadata":{},"cell_type":"markdown","source":"Out of all columns, below is the list of numeric columns\n\nTemperature,\nFuel_Price,\nMarkDown1,\nMarkDown2,\nMarkDown3,\nMarkDown4,\nMarkDown5,\nCPI,\nUnemployment"},{"metadata":{"trusted":true},"cell_type":"code","source":"## USe interactive backend\nmatplotlib.interactive(True)\n\nplt.ion()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for col in ['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']:\n    fig, axs = plt.subplots(1, 1)\n    axs.hist(raw_data[col].dropna())\n    axs.set_title('Distribution plot for '+col)\n    axs.set_xlabel(col)\n    axs.set_ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inferences by features\n\n##### Temperature\n1. Data is normally distributed. The dataset contains sales data across seasons.\n2. There are less number of outliers.\n3. Few outliers are with 0-20 degrees\n4. Depending on further analysis and while building model, decision to drop the outliers can be made\n\n##### Fuel Price \n1. Distribution is not normal. At the same time, outliers are limited too\n2. The data points are skewed.\n3. Upon further analysis, if the impact of Fuel prices on Sales is high, there could be interesting insights drawn and more analysis on how the fuel prices are influencing the sales will be useful.\n\nAt this point in time, not much can be commented about this\n\n##### Markdown1 to MArkdown 5\nData for all markdown fields is has common things in common\n1. Heavily skewed data with outliers\n2. It might be safe to assume to drop the outliers as their number is less.\n3. Proper hyp[othesis has to be done before dropping them\n\n##### CPI\nThere is a clear cluster in CPI distribution. Data can be assumed as categorical.\n\n##### unemployment\n1. Distribution is not normal. At the same time, outliers are limited too\n2. The data points are skewed.\n3. Upon further analysis, if the impact of Fuel prices on Sales is high, there could be interesting insights drawn and more analysis on how the unemployment is influencing the sales will be useful.\n\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"### Pair Plots\n\nsns pairplots give the distribution as well as bivariate releationships between various columns"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.set()\ncols = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\nsns.pairplot(raw_data[cols], size = 2.5)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Notable Inferences\n\n1. Weekly sales look to be highly skewed. The sales are very high during some days and relatively constant for the rest of the days\n2. CPI has 2 clusters\n3. Temperature is almost normally distributed across data. Indicates the data is collected over all kinds of whether conditions"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = raw_data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Heatmap\n\n1. There is a strong correlation between MArkdown 1 and Markdown 4 . One of the columns might be safe tobe excluded when using regression models\n2. Holiday and Markdown 2 and Markdown 3 have moderate correlation\n3. Other columns looks to be fairly uncorrelated"},{"metadata":{"nbpresent":{"id":"aaba4608-57bc-4567-8167-c2222d75cd8c"},"trusted":true},"cell_type":"code","source":"from IPython.display import IFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is theinteractive visualization with 3 pages. More details can be found clicking on the next previous buttons or the header\nFilters are also enabled and visualizations can be filtered by clicking on specific data points."},{"metadata":{"nbpresent":{"id":"dd859741-395e-4937-99cd-05f518fd4afb"},"scrolled":false,"trusted":true},"cell_type":"code","source":"\n%%html\n<div class='tableauPlaceholder' id='viz1569429520579' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Re&#47;RetailSales_15694295079860&#47;RetailSalesvariableanalysis&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='RetailSales_15694295079860&#47;RetailSalesvariableanalysis' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Re&#47;RetailSales_15694295079860&#47;RetailSalesvariableanalysis&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1569429520579');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='1016px';vizElement.style.height='991px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA Final words\n\nAlthough not many patterns are found from varioud graphs above, one interesting trend observed is there are clear clusters of sales spikes during Nov-Dec for years 2010 and 2011. However, the pattern is missing for 2012 which is interesting. \n\nMarkdowns are introduced only towards end of year 2011 (Nov-11-2011) and there seem to be no direct correlation between Markdown and Sales.\nMarkdowns also has many outliers.\n\nOne assumption to be made about Markdowns is to treat them as Promotions. But, there is no visible trend of Markdown vs Sales. \n\nEither, the assumption of Markdown being is wrong or the Promotions are not attractive or there could be other reasons which derived the lower sales in 2012"},{"metadata":{},"cell_type":"markdown","source":"### Missing Values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except markdowns, no other variable has missing values which is good.\n\nFor MArkdowns, almost half of the rows in the dataset has values missing.From the earlier analysis, the dataset has no markdowns until Nov-11-2011\n\nDropping the rows containing missing values may not be a good idea as it leads to information loss until Nov-11-2011\n\nOther options like imputing the missing values using KNN may be considered. (Dropping the ides of other imputing techniques like mean or median substitution because of non existant data for complete period prior Nov-11-2011)\n\nKNN imputing might be a reasonable technique in this scenario because it tries substitute the missing values based on nearest neighbours. If the data has a hidden pattern, then definitely substituting the missing values based on nearest neighbours would be more practical.\n\nOther approcah is to drop the Markdowns completely and build model out od the remaining dataset. "},{"metadata":{},"cell_type":"markdown","source":"#### KNN Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing Fancy impute\n#!pip install fancyimpute","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Delete Markdowns from dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_no_markdown = raw_data.drop([\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Other data cleaning techniques\n\nLeaving outliers as they are now. Decision on outliers can be taken when running the ML algorithms\n\nNot performing any feature scalings as there are outliers in come of the columns. Feature scaling on dataset containing outliers may result in highly bisased datasets\n\nDepending on the outcome of ML training, some of the variables may be converted to categorical and results may be compared.\nEg: Fuel price, CPI, Unemployment etc.,\n\nOther data transformation technique is to make a column with weeknumbers\n\nMore data cleaning can be done based on the ML algorithm used as not all algorithms are sussiptible to outliers and feature sclales and their effects vary by algorithm"},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"## Headsup for next steps\n\nContinuing to the intuition from EDA, one option to make good quality predictions is to build mnultiple models\n\nSome of them might be\n\n1. Build different models for markdown impouted dataset and for datasets with no markdowns\n2. As there is seasonality (Sales spikes at year end), it will be useful to build different models for Yearend alone (for all 3 years in single dataset) and different models for rest of the year (for all 3 years in single dataset)\n3. Perform feature scaling and deal outliers where ever required (Eg: Random forests and decision trees doesn't require feature scaling. But linear regresion require feature scaling)\n4. For models like linear regression, have to carefully look at correlation and perform PCA if needed.\n\nBesides all these, it is worth to look at building the time series as there is some seasonality observed at year end for all 3 years"},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"## Preparing Test dataset"},{"metadata":{},"cell_type":"markdown","source":"From the datasets, weekly_sales is present for data prior to 26-11-2012. Post this date, there is no data for weekly_sales and hence data after this qualifies to be good candidate for the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['features']['Date'] = pd.to_datetime(data['features']['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_weekly_sales_data = data['features'][data['features']['Date']>'2012-10-26']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.merge(no_weekly_sales_data,data['stores'], on='Store',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sales']['Date'] = pd.to_datetime(data['sales']['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.merge(test_data, data['sales'],  how='left', on=['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.to_pickle('test_data_no_dept.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_no_dept = raw_data.drop('Dept',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method to do basic data transforms and split the data\ndef transform_and_split(model_data, split_ratio=0.2):\n    model_data['Store'].astype('category')\n    model_data=pd.get_dummies(data=model_data,columns=['Type','IsHoliday'])\n    model_data['Date']=model_data['Date'].dt.week\n    sales = model_data['Weekly_Sales']\n    features = model_data.drop('Weekly_Sales', axis = 1)\n    # Shuffle and split the data into training and testing subsets\n    X_train, X_test, y_train, y_test = train_test_split(features,sales,test_size=split_ratio, random_state=1)\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model building\n\nApproach to modelling:\n\n1. Different models for Complete raw dataset (just a merge of all datasets provide) including markdown values and without any data transformation techniques other than replacing missing values of markdowns with zero.\n\n2. Non transformed dataset as above but withour markdown columns\n\n3. Apply some data transformations like replacing dates with week numbers and apply multiple algorithms\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Object to store all model details\nall_models = pd.DataFrame(columns=['Model', 'Description', 'r2', 'mae', 'rmse'])\nmodel_array = []\n# Utility function for saving model details\ndef store_model(desc, model, r2, mae, rmse):\n    model_data = {\n        'model': model,\n        'desc': desc,\n        'r2': r2,\n        'mae': mae,\n        'rmse': rmse\n    }\n    model_array.append(model_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndef rmse(predictions, targets):\n    return np.sqrt(((predictions - targets) ** 2).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef mae(predictions, targets):\n    return np.abs(np.sum(predictions - targets))/len(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_errors(model, y_test, X_test, do_print=False):\n    error_r2 = r2_score(y_test, model.predict(X_test))\n    error_mae = mae(y_test, model.predict(X_test))\n    error_rmse = rmse(y_test, model.predict(X_test))\n    if do_print:\n        print('Model details: ')\n        print(model)\n        print('r2: ' + str(error_r2))\n        print('mae: ' + str(error_mae))\n        print('rmse: ' + str(error_rmse))\n    return error_r2, error_mae, error_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1.1 - No data transformation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_only_markdown = raw_data[raw_data['Date'] >  '2011-10-26']\nraw_data_only_markdown.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_no_dept.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Linear Regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModelInterface:\n    def __init__(self, data, description ):\n        self.data = data\n        self.X_train, self.X_test, self.y_train, self.y_test = transform_and_split(data, 0.2)\n        self.model = None\n        self.description = description\n        self.r2 = None\n        self.mae = None\n        self.rmse = None\n        self.train_error_metrics: None\n        self.error_metrics: None\n    def transform_and_split(model_data, split_ratio=0.2):\n        model_data['Store'].astype('category')\n        model_data=pd.get_dummies(data=model_data,columns=['Type','IsHoliday'])\n        model_data['Date']=model_data['Date'].dt.week\n        sales = model_data['Weekly_Sales']\n        features = model_data.drop('Weekly_Sales', axis = 1)\n        # Shuffle and split the data into training and testing subsets\n        X_train, X_test, y_train, y_test = train_test_split(features,sales,test_size=split_ratio, random_state=1)\n        return X_train, X_test, y_train, y_test\n    def rmse(predictions, targets):\n        return np.sqrt(((predictions - targets) ** 2).mean())\n    def mae(predictions, targets):\n        return np.abs(np.sum(predictions - targets))/len(predictions)\n    def compute_errors(self, model):\n        error_r2 = r2_score(self.y_test, model.predict(self.X_test))\n        error_mae = mae(self.y_test, model.predict(self.X_test))\n        error_rmse = rmse(self.y_test, model.predict(self.X_test))\n        return error_r2, error_mae, error_rmse\n    def compute_train_errors(self, model):\n        error_r2 = r2_score(self.y_train, model.predict(self.X_train))\n        error_mae = mae(self.y_train, model.predict(self.X_train))\n        error_rmse = rmse(self.y_train, model.predict(self.X_train))\n        return error_r2, error_mae, error_rmse\n    def runModel(self, model1):\n        self.train_error_metrics = self.compute_train_errors(model1)\n        self.error_metrics = self.compute_errors(model1)\n    def printModel(self, model1):\n        print('Model details: ')\n        print(model1)\n        print('Description: '+ self.description)\n        print('train_r2: ' + str(self.train_error_metrics[0]))\n        print('train_mae: ' + str(self.train_error_metrics[1]))\n        print('train_rmse: ' + str(self.train_error_metrics[2]))\n        print('validation_r2: ' + str(self.error_metrics[0]))\n        print('validation_mae: ' + str(self.error_metrics[1]))\n        print('validation_rmse: ' + str(self.error_metrics[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = ModelInterface(raw_data_only_markdown, \"Linear regression Model with no data transformations. Includes all columns\")\nmodel1.model = LinearRegression(normalize=True).fit(model1.X_train,model1.y_train)\nmodel1.runModel(model1.model)\nmodel1.printModel(model1.model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import ShuffleSplit\n\ndef fit_model(algorithm, X, y, params):\n    \"\"\" Performs grid search over the 'max_depth' parameter for a \n        decision tree regressor trained on the input data [X, y]. \"\"\"\n    \n    # Create cross-validation sets from the training data\n    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)\n\n    # Create a decision tree regressor object\n    regressor = algorithm\n\n    # Transform 'performance_metric' into a scoring function using 'make_scorer' \n    scoring_fnc = make_scorer(r2_score)\n\n    # Create the grid search object\n    grid = GridSearchCV(regressor, param_grid= params, scoring=scoring_fnc, cv=cv_sets)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = ModelInterface(raw_data_only_markdown, \"Decision Tree regression Model with no data transformations. Includes all columns\")\nmodel2.model = DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best').fit(model2.X_train,model2.y_train)\nmodel2.runModel(model2.model)\nmodel2.printModel(model2.model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nmodel3 = ModelInterface(raw_data_only_markdown, \"Random forest regression Model with no data transformations. Includes all columns\")\nmodel3.model = RandomForestRegressor(n_estimators=100, criterion='mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=True, n_jobs=None, random_state=1, verbose=0, warm_start=False).fit(model3.X_train, model3.y_train)\nmodel3.runModel(model3.model)\nmodel3.printModel(model3.model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4 = ModelInterface(raw_data_no_markdown, \"Linear regression Model with no data transformations. Includes all columns except markdowns\")\nmodel4.model = LinearRegression(normalize=True).fit(model4.X_train,model4.y_train)\nmodel4.runModel(model4.model)\nmodel4.printModel(model4.model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model5 = ModelInterface(raw_data_no_markdown, \"Decision Tree regression Model with no data transformations. Includes all columns except markdowns\")\nmodel5.model = DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best').fit(model5.X_train,model5.y_train)\nmodel5.runModel(model5.model)\nmodel5.printModel(model5.model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model6 = ModelInterface(raw_data_no_markdown, \"Random forest regression Model with no data transformations. Includes all columns except markdowns\")\nmodel6.model = RandomForestRegressor(n_estimators=100, criterion='mse', \n                                     min_samples_split=2, min_samples_leaf=1, \n                                     min_weight_fraction_leaf=0.0, max_features='auto', \n                                     max_leaf_nodes=None, min_impurity_decrease=0.0, \n                                     min_impurity_split=None, bootstrap=True, oob_score=True, \n                                     n_jobs=None, random_state=1, verbose=0, warm_start=False).fit(model6.X_train, model6.y_train)\nmodel6.runModel(model6.model)\nmodel6.printModel(model6.model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model7 = ModelInterface(raw_data_no_dept, \"Decision Tree regression Model with no data transformations. Includes all columns including markdowns. This dataset contains data prior to the introduction of markdowns\")\nmodel7.model = DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best').fit(model7.X_train,model7.y_train)\nmodel7.runModel(model7.model)\nmodel7.printModel(model7.model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_models_pickle = {\n    'model1': model1,\n    'model2': model2,\n    'model3': model3,\n    'model4': model4,\n    'model5': model5,\n    'model6': model6,\n    'model7': model7\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\n#with open('/kaggle/working/all_models.pickle', 'wb') as handle:\n    #pickle.dump(all_models_pickle, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#with open('/kaggle/working/all_models.pickle', 'rb') as handle:\n    #ll_models = pickle.load(handle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"celltoolbar":"Slideshow","gist":{"data":{"description":"pdpbabi/git/pgpbabi/Capstone/RetailSales.ipynb","public":false},"id":""},"hide_input":false,"kernelspec":{"display_name":"conda_python3","language":"python","name":"conda_python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"number","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":true,"user_envs_cfg":false},"nbpresent":{"slides":{"badd8c9d-3520-4636-9693-34452231196e":{"id":"badd8c9d-3520-4636-9693-34452231196e","layout":"grid","prev":null,"regions":{"e1e2fe8c-39dd-4441-a14d-e6eb2691b6c9":{"attrs":{"height":0.8333333333333334,"pad":0.01,"width":0.8333333333333334,"x":0.08333333333333333,"y":0.08333333333333333},"id":"e1e2fe8c-39dd-4441-a14d-e6eb2691b6c9"}}}},"themes":{"default":"99602865-96d7-42c6-8876-7a6a3b854ec1","theme":{"99602865-96d7-42c6-8876-7a6a3b854ec1":{"backgrounds":{"dc7afa04-bf90-40b1-82a5-726e3cff5267":{"background-color":"31af15d2-7e15-44c5-ab5e-e04b16a89eff","id":"dc7afa04-bf90-40b1-82a5-726e3cff5267"}},"id":"99602865-96d7-42c6-8876-7a6a3b854ec1","palette":{"19cc588f-0593-49c9-9f4b-e4d7cc113b1c":{"id":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c","rgb":[252,252,252]},"31af15d2-7e15-44c5-ab5e-e04b16a89eff":{"id":"31af15d2-7e15-44c5-ab5e-e04b16a89eff","rgb":[68,68,68]},"50f92c45-a630-455b-aec3-788680ec7410":{"id":"50f92c45-a630-455b-aec3-788680ec7410","rgb":[197,226,245]},"c5cc3653-2ee1-402a-aba2-7caae1da4f6c":{"id":"c5cc3653-2ee1-402a-aba2-7caae1da4f6c","rgb":[43,126,184]},"efa7f048-9acb-414c-8b04-a26811511a21":{"id":"efa7f048-9acb-414c-8b04-a26811511a21","rgb":[25.118061674008803,73.60176211453744,107.4819383259912]}},"rules":{"a":{"color":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c"},"blockquote":{"color":"50f92c45-a630-455b-aec3-788680ec7410","font-size":3},"code":{"font-family":"Anonymous Pro"},"h1":{"color":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c","font-family":"Merriweather","font-size":8},"h2":{"color":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c","font-family":"Merriweather","font-size":6},"h3":{"color":"50f92c45-a630-455b-aec3-788680ec7410","font-family":"Lato","font-size":5.5},"h4":{"color":"c5cc3653-2ee1-402a-aba2-7caae1da4f6c","font-family":"Lato","font-size":5},"h5":{"font-family":"Lato"},"h6":{"font-family":"Lato"},"h7":{"font-family":"Lato"},"li":{"color":"50f92c45-a630-455b-aec3-788680ec7410","font-size":3.25},"pre":{"font-family":"Anonymous Pro","font-size":4}},"text-base":{"color":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c","font-family":"Lato","font-size":4}}}}},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"278px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"oldHeight":122,"position":{"height":"40px","left":"1069px","right":"20px","top":"120px","width":"350px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"varInspector_section_display":"none","window_display":false}},"nbformat":4,"nbformat_minor":1}