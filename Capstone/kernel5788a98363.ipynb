{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b1f19b73-6823-4ad0-8cb2-0432ec0fa508"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Retail Sales Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b5946145-61b4-42e0-a8f1-ebef17a22da9"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem Statement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Objective</a></span></li></ul></li><li><span><a href=\"#Data-description\" data-toc-modified-id=\"Data-description-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data description</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stores\" data-toc-modified-id=\"Stores-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Stores</a></span></li><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Features</a></span></li><li><span><a href=\"#Sales\" data-toc-modified-id=\"Sales-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Sales</a></span></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-and-Reading-the-datasets\" data-toc-modified-id=\"Importing-and-Reading-the-datasets-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Importing and Reading the datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initial-thoughts\" data-toc-modified-id=\"Initial-thoughts-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Initial thoughts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Steps-to-merge-the-datasets\" data-toc-modified-id=\"Steps-to-merge-the-datasets-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>Steps to merge the datasets</a></span></li><li><span><a href=\"#Final-dataset\" data-toc-modified-id=\"Final-dataset-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>Final dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Save-to-file\" data-toc-modified-id=\"Save-to-file-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Save to file</a></span><ul class=\"toc-item\"><li><span><a href=\"#CSV\" data-toc-modified-id=\"CSV-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>CSV</a></span></li><li><span><a href=\"#Pickle\" data-toc-modified-id=\"Pickle-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Pickle</a></span></li></ul></li><li><span><a href=\"#Data-description\" data-toc-modified-id=\"Data-description-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Data description</a></span></li><li><span><a href=\"#Univariate-Analysis\" data-toc-modified-id=\"Univariate-Analysis-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Univariate Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inferences-by-features\" data-toc-modified-id=\"Inferences-by-features-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Inferences by features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Temperature\" data-toc-modified-id=\"Temperature-3.4.1.1\"><span class=\"toc-item-num\">3.4.1.1&nbsp;&nbsp;</span>Temperature</a></span></li><li><span><a href=\"#Fuel-Price\" data-toc-modified-id=\"Fuel-Price-3.4.1.2\"><span class=\"toc-item-num\">3.4.1.2&nbsp;&nbsp;</span>Fuel Price</a></span></li><li><span><a href=\"#Markdown1-to-MArkdown-5\" data-toc-modified-id=\"Markdown1-to-MArkdown-5-3.4.1.3\"><span class=\"toc-item-num\">3.4.1.3&nbsp;&nbsp;</span>Markdown1 to MArkdown 5</a></span></li><li><span><a href=\"#CPI\" data-toc-modified-id=\"CPI-3.4.1.4\"><span class=\"toc-item-num\">3.4.1.4&nbsp;&nbsp;</span>CPI</a></span></li><li><span><a href=\"#unemployment\" data-toc-modified-id=\"unemployment-3.4.1.5\"><span class=\"toc-item-num\">3.4.1.5&nbsp;&nbsp;</span>unemployment</a></span></li></ul></li></ul></li><li><span><a href=\"#Pair-Plots\" data-toc-modified-id=\"Pair-Plots-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Pair Plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notable-Inferences\" data-toc-modified-id=\"Notable-Inferences-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Notable Inferences</a></span></li></ul></li><li><span><a href=\"#Correlation-Heatmap\" data-toc-modified-id=\"Correlation-Heatmap-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Correlation Heatmap</a></span></li><li><span><a href=\"#EDA-Final-words\" data-toc-modified-id=\"EDA-Final-words-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>EDA Final words</a></span></li><li><span><a href=\"#Missing-Values\" data-toc-modified-id=\"Missing-Values-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Missing Values</a></span><ul class=\"toc-item\"><li><span><a href=\"#KNN-Imputation\" data-toc-modified-id=\"KNN-Imputation-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>KNN Imputation</a></span></li><li><span><a href=\"#Delete-Markdowns-from-dataset\" data-toc-modified-id=\"Delete-Markdowns-from-dataset-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span>Delete Markdowns from dataset</a></span></li></ul></li><li><span><a href=\"#Other-data-cleaning-techniques\" data-toc-modified-id=\"Other-data-cleaning-techniques-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Other data cleaning techniques</a></span></li></ul></li><li><span><a href=\"#Headsup-for-next-steps\" data-toc-modified-id=\"Headsup-for-next-steps-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Headsup for next steps</a></span></li><li><span><a href=\"#Preparing-Test-dataset\" data-toc-modified-id=\"Preparing-Test-dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Preparing Test dataset</a></span></li><li><span><a href=\"#Model-1---No-data-transformation\" data-toc-modified-id=\"Model-1---No-data-transformation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model 1 - No data transformation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "e109f388-76aa-4f10-8f63-9ce120093ffd"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "This project is to predict the sales across different stores. \n",
    "The data containing historical sales data for 45 stores located in different regions - each store contains a\n",
    "number of departments. The company also runs several promotional markdown events throughout the\n",
    "year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor\n",
    "Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in\n",
    "the evaluation than non-holiday weeks. Within the Excel Sheet, there are 3 Tabs – Stores, Features and\n",
    "Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c52fcedc-f32c-446f-af38-bf5705888930"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective\n",
    "\n",
    "The objective of this project is to \n",
    "\n",
    "1. Predict the department-wide sales for each store for the following year\n",
    "\n",
    "2. Model the effects of markdowns on holiday weeks\n",
    "\n",
    "3. Provide recommended actions based on the insights drawn, with prioritization placed on largest business\n",
    "impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a58230cd-497e-47c0-a909-75b97d2c191c"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data description\n",
    "\n",
    "There are 3 datasets used in this project. Their respecitve details as follows:\n",
    "\n",
    "### Stores\n",
    "\n",
    "Anonymized information about the 45 stores, indicating the type and size of store\n",
    "\n",
    "Store - Store ID\n",
    "\n",
    "Type - Type of Store (A,B,C)\n",
    "\n",
    "Size – Size of the store\n",
    "\n",
    "### Features\n",
    "\n",
    "Contains additional data related to the store, department, and regional activity for the given dates.\n",
    "\n",
    "Store – Store ID\n",
    "\n",
    "Date – Week start date\n",
    "\n",
    "Temperature - average temperature in the region\n",
    "\n",
    "Fuel_Price - cost of fuel in the region\n",
    "\n",
    "MarkDown1-5 - anonymized data related to promotional markdowns. MarkDown data is only available\n",
    "after Nov 2011, and is not available for all stores all the time. Any missing value is marked with NA\n",
    "\n",
    "CPI - the consumer price index\n",
    "\n",
    "Unemployment - the unemployment rate\n",
    "\n",
    "IsHoliday - whether the week is a special holiday week\n",
    "\n",
    "### Sales\n",
    "\n",
    "Historical sales data, which covers to 2010-02-05 to 2012-11-01. Within this tab you will find the\n",
    "following fields:\n",
    "\n",
    "Store – Store ID\n",
    "\n",
    "Dept – Department ID\n",
    "\n",
    "Date – Week start date\n",
    "\n",
    "Weekly_Sales -sales for the given department in the given store\n",
    "\n",
    "IsHoliday - whether the week is a special holiday week\n",
    "\n",
    "The dataset contains weekly sales data over the period of 3 years. Although data is clean for many features, there are mission values for Markdown columns anbd no data description about what these columns are. There could be challenges in understanding these columns and accommodating them to fit into the model along with handling lot of missing values there columns poccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Reading the datasets\n",
    "\n",
    "The data is located in 3 csv files \n",
    "\n",
    "Sales - Retail Sales Prediction-Sales data-set.csv\n",
    "\n",
    "Features - Retail Sales Prediction-Features data set.csv\n",
    "\n",
    "Stores - Retail Sales Prediction-Stores data-set.csv\n",
    "\n",
    "Overall steps for importing and reading the datasets are:\n",
    "\n",
    "1. import pandas - very useful for the data analysis\n",
    "2. Create a dictionary which holds the data read from all datasets - Dictionary is used to hold the data as a single object. This can be very convinient instead of creatig multiple variables\n",
    "3. For each csv file, use pandas read.csv method by inputting the relevant file name\n",
    "\n",
    "All the data read will be stored as 3 different dataframes within a single dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false,
    "nbpresent": {
     "id": "717f30fd-42c5-469d-9d07-acb26c7cbee8"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "hide_input": false,
    "nbpresent": {
     "id": "f1407ebf-389c-41a1-a508-922b3b626f7a"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Store all the csv as python dataframes. Storing in single variable for convenience\n",
    "data={\n",
    "    \"sales\": pd.read_csv('Retail Sales Prediction-Sales data-set.csv'),\n",
    "    \"features\": pd.read_csv('Retail Sales Prediction-Features data set.csv'),\n",
    "    \"stores\": pd.read_csv('Retail Sales Prediction-Stores data-set.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  Dept        Date  Weekly_Sales  IsHoliday\n",
      "0      1     1  05/02/2010      24924.50      False\n",
      "1      1     1  12/02/2010      46039.49       True\n",
      "2      1     1  19/02/2010      41595.55      False\n",
      "3      1     1  26/02/2010      19403.54      False\n",
      "4      1     1  05/03/2010      21827.90      False\n",
      "421570\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data['sales'].head())\n",
    "print(len(data['sales']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "a7ed16e3-dab8-48df-a54d-019b4384fec9"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store Type    Size\n",
      "0      1    A  151315\n",
      "1      2    A  202307\n",
      "2      3    B   37392\n",
      "3      4    A  205863\n",
      "4      5    B   34875\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data['stores'].head())\n",
    "print(len(data['stores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "1c108fce-c04e-400c-b95d-0e90e879da23"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
      "0      1  05/02/2010        42.31       2.572        NaN        NaN   \n",
      "1      1  12/02/2010        38.51       2.548        NaN        NaN   \n",
      "2      1  19/02/2010        39.93       2.514        NaN        NaN   \n",
      "3      1  26/02/2010        46.63       2.561        NaN        NaN   \n",
      "4      1  05/03/2010        46.50       2.625        NaN        NaN   \n",
      "\n",
      "   MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
      "0        NaN        NaN        NaN  211.096358         8.106      False  \n",
      "1        NaN        NaN        NaN  211.242170         8.106       True  \n",
      "2        NaN        NaN        NaN  211.289143         8.106      False  \n",
      "3        NaN        NaN        NaN  211.319643         8.106      False  \n",
      "4        NaN        NaN        NaN  211.350143         8.106      False  \n",
      "8190\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data['features'].head())\n",
    "print(len(data['features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial thoughts\n",
    "\n",
    "Upon inspecting the data from 3 datasets, its evident that \n",
    "\n",
    "1. Sales data contains historical data\n",
    "2. Stores contain the information regarding stores. This dataset doesn't add value on its own but combining store information with sales information helps in understanding the releation between sales across different type of stores\n",
    "3. Features dataset contains sales information with aditional features. \n",
    "\n",
    "Upon carefully obeserving Features and Sales, its evident that there are few variables in Sales and Features in common and features has few addititional variables. Combining all 3 datasets helps in having a simplified single dataset and it helps leveraging more value out of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps to merge the datasets\n",
    "\n",
    "1. Join any 2 datasets based on the common columns. This can be similar to Left join in SQL\n",
    "2. With the dataframe resulting from step 1, join the 3rd dataframe following similar approach as Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "nbpresent": {
     "id": "fec2fd2e-9b3d-4c39-8b17-c063e65d8143"
    }
   },
   "outputs": [],
   "source": [
    "# this df stores the merged df og features and stores csv\n",
    "#data['merged'] = pd.merge(data['features'],data['stores'], on='Store',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "nbpresent": {
     "id": "3b1705ce-07cc-4bf6-9125-7a3a33c5ce08"
    }
   },
   "outputs": [],
   "source": [
    "# This df contains all 3 df joined int one\n",
    "#data['merged_total'] = pd.merge(data['sales'], data['merged'],  how='left', left_on=['Store','Date','IsHoliday'], right_on = ['Store','Date','IsHoliday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "31d2282f-a335-463b-a69a-5c1fbc5982ad"
    }
   },
   "outputs": [],
   "source": [
    "#raw_data = data['merged_total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final dataset\n",
    "\n",
    "As a result of merging all 3 dataframes, final dataset has a total of 421570 observations and 16 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "f5c8b64e-97fb-4672-bb3b-a066af62d5e2"
    }
   },
   "outputs": [],
   "source": [
    "#raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file\n",
    "\n",
    "Save the merged dataframe to a csv file and pickle for future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV\n",
    "\n",
    "File can be saved as csv using pandas to_csv method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "5a40355e-45bd-4d80-9e4d-af7161de2957"
    }
   },
   "outputs": [],
   "source": [
    "#raw_data.to_csv('Retail_Sales_Merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle\n",
    "\n",
    "Pickle is python's way to store the dataframes as python objects. Saving any dataframe to pickle helps in resuing the pickle object directly to read the dataframe to session\n",
    "\n",
    "This file is stored on the disk and can be exported to any file storage sevice thus giving the flexibility resuing the object to read directly from storage locations when there is change in working environments.\n",
    "\n",
    "Also, pickling helps to avoid all the data reading steps from csv and all steps involved in merging dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_pickle('Retail_Sales_Merged.pkl')\n",
    "raw_data['Date'] = pd.to_datetime(raw_data['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Store</th>\n",
       "      <td>421570.0</td>\n",
       "      <td>22.200546</td>\n",
       "      <td>12.785297</td>\n",
       "      <td>1.000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>22.00000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dept</th>\n",
       "      <td>421570.0</td>\n",
       "      <td>44.260317</td>\n",
       "      <td>30.492054</td>\n",
       "      <td>1.000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <td>421570.0</td>\n",
       "      <td>15981.258123</td>\n",
       "      <td>22711.183519</td>\n",
       "      <td>-4988.940</td>\n",
       "      <td>2079.650000</td>\n",
       "      <td>7612.03000</td>\n",
       "      <td>20205.852500</td>\n",
       "      <td>693099.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Temperature</th>\n",
       "      <td>421570.0</td>\n",
       "      <td>60.090059</td>\n",
       "      <td>18.447931</td>\n",
       "      <td>-2.060</td>\n",
       "      <td>46.680000</td>\n",
       "      <td>62.09000</td>\n",
       "      <td>74.280000</td>\n",
       "      <td>100.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fuel_Price</th>\n",
       "      <td>421570.0</td>\n",
       "      <td>3.361027</td>\n",
       "      <td>0.458515</td>\n",
       "      <td>2.472</td>\n",
       "      <td>2.933000</td>\n",
       "      <td>3.45200</td>\n",
       "      <td>3.738000</td>\n",
       "      <td>4.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MarkDown1</th>\n",
       "      <td>150681.0</td>\n",
       "      <td>7246.420196</td>\n",
       "      <td>8291.221345</td>\n",
       "      <td>0.270</td>\n",
       "      <td>2240.270000</td>\n",
       "      <td>5347.45000</td>\n",
       "      <td>9210.900000</td>\n",
       "      <td>88646.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MarkDown2</th>\n",
       "      <td>111248.0</td>\n",
       "      <td>3334.628621</td>\n",
       "      <td>9475.357325</td>\n",
       "      <td>-265.760</td>\n",
       "      <td>41.600000</td>\n",
       "      <td>192.00000</td>\n",
       "      <td>1926.940000</td>\n",
       "      <td>104519.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MarkDown3</th>\n",
       "      <td>137091.0</td>\n",
       "      <td>1439.421384</td>\n",
       "      <td>9623.078290</td>\n",
       "      <td>-29.100</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>24.60000</td>\n",
       "      <td>103.990000</td>\n",
       "      <td>141630.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MarkDown4</th>\n",
       "      <td>134967.0</td>\n",
       "      <td>3383.168256</td>\n",
       "      <td>6292.384031</td>\n",
       "      <td>0.220</td>\n",
       "      <td>504.220000</td>\n",
       "      <td>1481.31000</td>\n",
       "      <td>3595.040000</td>\n",
       "      <td>67474.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MarkDown5</th>\n",
       "      <td>151432.0</td>\n",
       "      <td>4628.975079</td>\n",
       "      <td>5962.887455</td>\n",
       "      <td>135.160</td>\n",
       "      <td>1878.440000</td>\n",
       "      <td>3359.45000</td>\n",
       "      <td>5563.800000</td>\n",
       "      <td>108519.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPI</th>\n",
       "      <td>421570.0</td>\n",
       "      <td>171.201947</td>\n",
       "      <td>39.159276</td>\n",
       "      <td>126.064</td>\n",
       "      <td>132.022667</td>\n",
       "      <td>182.31878</td>\n",
       "      <td>212.416993</td>\n",
       "      <td>227.232807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unemployment</th>\n",
       "      <td>421570.0</td>\n",
       "      <td>7.960289</td>\n",
       "      <td>1.863296</td>\n",
       "      <td>3.879</td>\n",
       "      <td>6.891000</td>\n",
       "      <td>7.86600</td>\n",
       "      <td>8.572000</td>\n",
       "      <td>14.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Size</th>\n",
       "      <td>421570.0</td>\n",
       "      <td>136727.915739</td>\n",
       "      <td>60980.583328</td>\n",
       "      <td>34875.000</td>\n",
       "      <td>93638.000000</td>\n",
       "      <td>140167.00000</td>\n",
       "      <td>202505.000000</td>\n",
       "      <td>219622.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count           mean           std        min           25%  \\\n",
       "Store         421570.0      22.200546     12.785297      1.000     11.000000   \n",
       "Dept          421570.0      44.260317     30.492054      1.000     18.000000   \n",
       "Weekly_Sales  421570.0   15981.258123  22711.183519  -4988.940   2079.650000   \n",
       "Temperature   421570.0      60.090059     18.447931     -2.060     46.680000   \n",
       "Fuel_Price    421570.0       3.361027      0.458515      2.472      2.933000   \n",
       "MarkDown1     150681.0    7246.420196   8291.221345      0.270   2240.270000   \n",
       "MarkDown2     111248.0    3334.628621   9475.357325   -265.760     41.600000   \n",
       "MarkDown3     137091.0    1439.421384   9623.078290    -29.100      5.080000   \n",
       "MarkDown4     134967.0    3383.168256   6292.384031      0.220    504.220000   \n",
       "MarkDown5     151432.0    4628.975079   5962.887455    135.160   1878.440000   \n",
       "CPI           421570.0     171.201947     39.159276    126.064    132.022667   \n",
       "Unemployment  421570.0       7.960289      1.863296      3.879      6.891000   \n",
       "Size          421570.0  136727.915739  60980.583328  34875.000  93638.000000   \n",
       "\n",
       "                       50%            75%            max  \n",
       "Store             22.00000      33.000000      45.000000  \n",
       "Dept              37.00000      74.000000      99.000000  \n",
       "Weekly_Sales    7612.03000   20205.852500  693099.360000  \n",
       "Temperature       62.09000      74.280000     100.140000  \n",
       "Fuel_Price         3.45200       3.738000       4.468000  \n",
       "MarkDown1       5347.45000    9210.900000   88646.760000  \n",
       "MarkDown2        192.00000    1926.940000  104519.540000  \n",
       "MarkDown3         24.60000     103.990000  141630.610000  \n",
       "MarkDown4       1481.31000    3595.040000   67474.850000  \n",
       "MarkDown5       3359.45000    5563.800000  108519.280000  \n",
       "CPI              182.31878     212.416993     227.232807  \n",
       "Unemployment       7.86600       8.572000      14.313000  \n",
       "Size          140167.00000  202505.000000  219622.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4717d7a8058f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nbagg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store                    int64\n",
       "Dept                     int64\n",
       "Date            datetime64[ns]\n",
       "Weekly_Sales           float64\n",
       "IsHoliday                 bool\n",
       "Temperature            float64\n",
       "Fuel_Price             float64\n",
       "MarkDown1              float64\n",
       "MarkDown2              float64\n",
       "MarkDown3              float64\n",
       "MarkDown4              float64\n",
       "MarkDown5              float64\n",
       "CPI                    float64\n",
       "Unemployment           float64\n",
       "Type                    object\n",
       "Size                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all numeric columns, Store, Dept and Size seem categorical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all columns, below is the list of numeric columns\n",
    "\n",
    "Temperature,\n",
    "Fuel_Price,\n",
    "MarkDown1,\n",
    "MarkDown2,\n",
    "MarkDown3,\n",
    "MarkDown4,\n",
    "MarkDown5,\n",
    "CPI,\n",
    "Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USe interactive backend\n",
    "matplotlib.interactive(True)\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in ['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']:\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.hist(raw_data[col].dropna())\n",
    "    axs.set_title('Distribution plot for '+col)\n",
    "    axs.set_xlabel(col)\n",
    "    axs.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences by features\n",
    "\n",
    "##### Temperature\n",
    "1. Data is normally distributed. The dataset contains sales data across seasons.\n",
    "2. There are less number of outliers.\n",
    "3. Few outliers are with 0-20 degrees\n",
    "4. Depending on further analysis and while building model, decision to drop the outliers can be made\n",
    "\n",
    "##### Fuel Price \n",
    "1. Distribution is not normal. At the same time, outliers are limited too\n",
    "2. The data points are skewed.\n",
    "3. Upon further analysis, if the impact of Fuel prices on Sales is high, there could be interesting insights drawn and more analysis on how the fuel prices are influencing the sales will be useful.\n",
    "\n",
    "At this point in time, not much can be commented about this\n",
    "\n",
    "##### Markdown1 to MArkdown 5\n",
    "Data for all markdown fields is has common things in common\n",
    "1. Heavily skewed data with outliers\n",
    "2. It might be safe to assume to drop the outliers as their number is less.\n",
    "3. Proper hyp[othesis has to be done before dropping them\n",
    "\n",
    "##### CPI\n",
    "There is a clear cluster in CPI distribution. Data can be assumed as categorical.\n",
    "\n",
    "##### unemployment\n",
    "1. Distribution is not normal. At the same time, outliers are limited too\n",
    "2. The data points are skewed.\n",
    "3. Upon further analysis, if the impact of Fuel prices on Sales is high, there could be interesting insights drawn and more analysis on how the unemployment is influencing the sales will be useful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pair Plots\n",
    "\n",
    "sns pairplots give the distribution as well as bivariate releationships between various columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "cols = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "sns.pairplot(raw_data[cols], size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notable Inferences\n",
    "\n",
    "1. Weekly sales look to be highly skewed. The sales are very high during some days and relatively constant for the rest of the days\n",
    "2. CPI has 2 clusters\n",
    "3. Temperature is almost normally distributed across data. Indicates the data is collected over all kinds of whether conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0c784219a06e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorrmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "corrmat = raw_data.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap\n",
    "\n",
    "1. There is a strong correlation between MArkdown 1 and Markdown 4 . One of the columns might be safe tobe excluded when using regression models\n",
    "2. Holiday and Markdown 2 and Markdown 3 have moderate correlation\n",
    "3. Other columns looks to be fairly uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "aaba4608-57bc-4567-8167-c2222d75cd8c"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is theinteractive visualization with 3 pages. More details can be found clicking on the next previous buttons or the header\n",
    "Filters are also enabled and visualizations can be filtered by clicking on specific data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "dd859741-395e-4937-99cd-05f518fd4afb"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-ac6c86adb0df>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-ac6c86adb0df>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    <div class='tableauPlaceholder' id='viz1569429520579' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Re&#47;RetailSales_15694295079860&#47;RetailSalesvariableanalysis&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='RetailSales_15694295079860&#47;RetailSalesvariableanalysis' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Re&#47;RetailSales_15694295079860&#47;RetailSalesvariableanalysis&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1569429520579');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='1016px';vizElement.style.height='991px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%html\n",
    "<div class='tableauPlaceholder' id='viz1569429520579' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Re&#47;RetailSales_15694295079860&#47;RetailSalesvariableanalysis&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='RetailSales_15694295079860&#47;RetailSalesvariableanalysis' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Re&#47;RetailSales_15694295079860&#47;RetailSalesvariableanalysis&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1569429520579');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='1016px';vizElement.style.height='991px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Final words\n",
    "\n",
    "Although not many patterns are found from varioud graphs above, one interesting trend observed is there are clear clusters of sales spikes during Nov-Dec for years 2010 and 2011. However, the pattern is missing for 2012 which is interesting. \n",
    "\n",
    "Markdowns are introduced only towards end of year 2011 (Nov-11-2011) and there seem to be no direct correlation between Markdown and Sales.\n",
    "Markdowns also has many outliers.\n",
    "\n",
    "One assumption to be made about Markdowns is to treat them as Promotions. But, there is no visible trend of Markdown vs Sales. \n",
    "\n",
    "Either, the assumption of Markdown being is wrong or the Promotions are not attractive or there could be other reasons which derived the lower sales in 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store                0\n",
       "Dept                 0\n",
       "Date                 0\n",
       "Weekly_Sales         0\n",
       "IsHoliday            0\n",
       "Temperature          0\n",
       "Fuel_Price           0\n",
       "MarkDown1       270889\n",
       "MarkDown2       310322\n",
       "MarkDown3       284479\n",
       "MarkDown4       286603\n",
       "MarkDown5       270138\n",
       "CPI                  0\n",
       "Unemployment         0\n",
       "Type                 0\n",
       "Size                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except markdowns, no other variable has missing values which is good.\n",
    "\n",
    "For MArkdowns, almost half of the rows in the dataset has values missing.From the earlier analysis, the dataset has no markdowns until Nov-11-2011\n",
    "\n",
    "Dropping the rows containing missing values may not be a good idea as it leads to information loss until Nov-11-2011\n",
    "\n",
    "Other options like imputing the missing values using KNN may be considered. (Dropping the ides of other imputing techniques like mean or median substitution because of non existant data for complete period prior Nov-11-2011)\n",
    "\n",
    "KNN imputing might be a reasonable technique in this scenario because it tries substitute the missing values based on nearest neighbours. If the data has a hidden pattern, then definitely substituting the missing values based on nearest neighbours would be more practical.\n",
    "\n",
    "Other approcah is to drop the Markdowns completely and build model out od the remaining dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Fancy impute\n",
    "#!pip install fancyimpute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Markdowns from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_no_markdown = raw_data.drop([\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other data cleaning techniques\n",
    "\n",
    "Leaving outliers as they are now. Decision on outliers can be taken when running the ML algorithms\n",
    "\n",
    "Not performing any feature scalings as there are outliers in come of the columns. Feature scaling on dataset containing outliers may result in highly bisased datasets\n",
    "\n",
    "Depending on the outcome of ML training, some of the variables may be converted to categorical and results may be compared.\n",
    "Eg: Fuel price, CPI, Unemployment etc.,\n",
    "\n",
    "Other data transformation technique is to make a column with weeknumbers\n",
    "\n",
    "More data cleaning can be done based on the ML algorithm used as not all algorithms are sussiptible to outliers and feature sclales and their effects vary by algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Headsup for next steps\n",
    "\n",
    "Continuing to the intuition from EDA, one option to make good quality predictions is to build mnultiple models\n",
    "\n",
    "Some of them might be\n",
    "\n",
    "1. Build different models for markdown impouted dataset and for datasets with no markdowns\n",
    "2. As there is seasonality (Sales spikes at year end), it will be useful to build different models for Yearend alone (for all 3 years in single dataset) and different models for rest of the year (for all 3 years in single dataset)\n",
    "3. Perform feature scaling and deal outliers where ever required (Eg: Random forests and decision trees doesn't require feature scaling. But linear regresion require feature scaling)\n",
    "4. For models like linear regression, have to carefully look at correlation and perform PCA if needed.\n",
    "\n",
    "Besides all these, it is worth to look at building the time series as there is some seasonality observed at year end for all 3 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preparing Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the datasets, weekly_sales is present for data prior to 26-11-2012. Post this date, there is no data for weekly_sales and hence data after this qualifies to be good candidate for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['features']['Date'] = pd.to_datetime(data['features']['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_weekly_sales_data = data['features'][data['features']['Date']>'2012-10-26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.merge(no_weekly_sales_data,data['stores'], on='Store',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sales']['Date'] = pd.to_datetime(data['sales']['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.merge(test_data, data['sales'],  how='left', on=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_pickle('test_data_no_dept.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_no_dept = raw_data.drop('Dept',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to do basic data transforms and split the data\n",
    "def transform_and_split(model_data, split_ratio=0.2):\n",
    "    model_data['Store'].astype('category')\n",
    "    model_data=pd.get_dummies(data=model_data,columns=['Type','IsHoliday'])\n",
    "    model_data['Date']=model_data['Date'].dt.week\n",
    "    sales = model_data['Weekly_Sales']\n",
    "    features = model_data.drop('Weekly_Sales', axis = 1)\n",
    "    # Shuffle and split the data into training and testing subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,sales,test_size=split_ratio, random_state=1)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "Approach to modelling:\n",
    "\n",
    "1. Different models for Complete raw dataset (just a merge of all datasets provide) including markdown values and without any data transformation techniques other than replacing missing values of markdowns with zero.\n",
    "\n",
    "2. Non transformed dataset as above but withour markdown columns\n",
    "\n",
    "3. Apply some data transformations like replacing dates with week numbers and apply multiple algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.09335136190344195, 4.4241154264012437e-13, 22072.269683556686)\n",
      "(0.09415920290341961, 91.08942341878617, 21751.242044027294)\n"
     ]
    }
   ],
   "source": [
    "print(model1.train_error_metrics)\n",
    "print(model1.error_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object to store all model details\n",
    "all_models = pd.DataFrame(columns=['Model', 'Description', 'train_r2', 'validation_r2', 'train_mae', 'validation_mae', 'train_rmse', 'validation_rmse'])\n",
    "model_array = []\n",
    "# Utility function for saving model details\n",
    "def store_model(modelInterface):\n",
    "    model_data = {\n",
    "        'Model': modelInterface.model,\n",
    "        'Description': modelInterface.description,\n",
    "        'train_r2': modelInterface.train_error_metrics[0],\n",
    "        'validation_r2':modelInterface.error_metrics[0],\n",
    "        'train_mae': modelInterface.train_error_metrics[1],\n",
    "        'validation_mae': modelInterface.error_metrics[1],\n",
    "        'train_rmse': modelInterface.train_error_metrics[2],\n",
    "        'validation_rmse': modelInterface.error_metrics[2]\n",
    "    }\n",
    "    model_array.append(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mae(predictions, targets):\n",
    "    return np.abs(np.sum(predictions - targets))/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(model, y_test, X_test, do_print=False):\n",
    "    error_r2 = r2_score(y_test, model.predict(X_test))\n",
    "    error_mae = mae(y_test, model.predict(X_test))\n",
    "    error_rmse = rmse(y_test, model.predict(X_test))\n",
    "    if do_print:\n",
    "        print('Model details: ')\n",
    "        print(model)\n",
    "        print('r2: ' + str(error_r2))\n",
    "        print('mae: ' + str(error_mae))\n",
    "        print('rmse: ' + str(error_rmse))\n",
    "    return error_r2, error_mae, error_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.1 - No data transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/pandas/core/frame.py:3790: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "raw_data_only_markdown = raw_data[raw_data['Date'] >  '2011-10-26']\n",
    "raw_data_only_markdown.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_no_dept.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.7MB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/50/a552a5aff252ae915f522e44642bb49a7b7b31677f9580cfd11bcc869976/scipy-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 25.2MB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.1MB 4.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from scikit-learn) (1.15.4)\n",
      "Collecting joblib>=0.11 (from scikit-learn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 40.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from matplotlib) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/fastai/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.6.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Installing collected packages: joblib, scipy, scikit-learn, matplotlib\n",
      "  Found existing installation: scipy 1.2.0\n",
      "    Uninstalling scipy-1.2.0:\n",
      "      Successfully uninstalled scipy-1.2.0\n",
      "  Found existing installation: matplotlib 3.0.2\n",
      "    Uninstalling matplotlib-3.0.2:\n",
      "      Successfully uninstalled matplotlib-3.0.2\n",
      "Successfully installed joblib-0.14.0 matplotlib-3.1.1 scikit-learn-0.21.3 scipy-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInterface:\n",
    "    def __init__(self, data, description ):\n",
    "        self.data = data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = transform_and_split(data, 0.2)\n",
    "        self.model = None\n",
    "        self.description = description\n",
    "        self.r2 = None\n",
    "        self.mae = None\n",
    "        self.rmse = None\n",
    "        self.train_error_metrics: None\n",
    "        self.error_metrics: None\n",
    "    def transform_and_split(model_data, split_ratio=0.2):\n",
    "        model_data['Store'].astype('category')\n",
    "        model_data=pd.get_dummies(data=model_data,columns=['Type','IsHoliday'])\n",
    "        model_data['Date']=model_data['Date'].dt.week\n",
    "        sales = model_data['Weekly_Sales']\n",
    "        features = model_data.drop('Weekly_Sales', axis = 1)\n",
    "        # Shuffle and split the data into training and testing subsets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features,sales,test_size=split_ratio, random_state=1)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    def rmse(predictions, targets):\n",
    "        return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "    def mae(predictions, targets):\n",
    "        return np.abs(np.sum(predictions - targets))/len(predictions)\n",
    "    def compute_errors(self, model):\n",
    "        error_r2 = r2_score(self.y_test, model.predict(self.X_test))\n",
    "        error_mae = mae(self.y_test, model.predict(self.X_test))\n",
    "        error_rmse = rmse(self.y_test, model.predict(self.X_test))\n",
    "        return error_r2, error_mae, error_rmse\n",
    "    def compute_train_errors(self, model):\n",
    "        error_r2 = r2_score(self.y_train, model.predict(self.X_train))\n",
    "        error_mae = mae(self.y_train, model.predict(self.X_train))\n",
    "        error_rmse = rmse(self.y_train, model.predict(self.X_train))\n",
    "        return error_r2, error_mae, error_rmse\n",
    "    def runModel(self, model1):\n",
    "        self.train_error_metrics = self.compute_train_errors(model1)\n",
    "        self.error_metrics = self.compute_errors(model1)\n",
    "    def printModel(self, model1):\n",
    "        print('Model details: ')\n",
    "        print(model1)\n",
    "        print('Description: '+ self.description)\n",
    "        print('train_r2: ' + str(self.train_error_metrics[0]))\n",
    "        print('train_mae: ' + str(self.train_error_metrics[1]))\n",
    "        print('train_rmse: ' + str(self.train_error_metrics[2]))\n",
    "        print('validation_r2: ' + str(self.error_metrics[0]))\n",
    "        print('validation_mae: ' + str(self.error_metrics[1]))\n",
    "        print('validation_rmse: ' + str(self.error_metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: \n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)\n",
      "Description: Linear regression Model with no data transformations. Includes all columns\n",
      "train_r2: 0.09335136190344195\n",
      "train_mae: 4.4241154264012437e-13\n",
      "train_rmse: 22072.269683556686\n",
      "validation_r2: 0.09415920290341961\n",
      "validation_mae: 91.08942341878617\n",
      "validation_rmse: 21751.242044027294\n"
     ]
    }
   ],
   "source": [
    "model1 = ModelInterface(raw_data_only_markdown, \"Linear regression Model with no data transformations. Includes all columns\")\n",
    "model1.model = LinearRegression(normalize=True).fit(model1.X_train,model1.y_train)\n",
    "model1.runModel(model1.model)\n",
    "model1.printModel(model1.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def fit_model(algorithm, X, y, params):\n",
    "    \"\"\" Performs grid search over the 'max_depth' parameter for a \n",
    "        decision tree regressor trained on the input data [X, y]. \"\"\"\n",
    "    \n",
    "    # Create cross-validation sets from the training data\n",
    "    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)\n",
    "\n",
    "    # Create a decision tree regressor object\n",
    "    regressor = algorithm\n",
    "\n",
    "    # Transform 'performance_metric' into a scoring function using 'make_scorer' \n",
    "    scoring_fnc = make_scorer(r2_score)\n",
    "\n",
    "    # Create the grid search object\n",
    "    grid = GridSearchCV(regressor, param_grid= params, scoring=scoring_fnc, cv=cv_sets)\n",
    "\n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: \n",
      "DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "Description: Decision Tree regression Model with no data transformations. Includes all columns\n",
      "train_r2: 0.7608430604905764\n",
      "train_mae: 3.4509025873090033e-12\n",
      "train_rmse: 11336.238663526812\n",
      "validation_r2: 0.7591936136791231\n",
      "validation_mae: 51.81069297940961\n",
      "validation_rmse: 11214.815241125414\n"
     ]
    }
   ],
   "source": [
    "model2 = ModelInterface(raw_data_only_markdown, \"Decision Tree regression Model with no data transformations. Includes all columns\")\n",
    "model2.model = DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n",
    "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=1,\n",
    "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "           presort=False, random_state=None, splitter='best').fit(model2.X_train,model2.y_train)\n",
    "model2.runModel(model2.model)\n",
    "model2.printModel(model2.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: \n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                      n_jobs=None, oob_score=True, random_state=1, verbose=0,\n",
      "                      warm_start=False)\n",
      "Description: Random forest regression Model with no data transformations. Includes all columns\n",
      "train_r2: 0.9949871119356408\n",
      "train_mae: 19.208004602480617\n",
      "train_rmse: 1641.2377235868435\n",
      "validation_r2: 0.969933597866702\n",
      "validation_mae: 52.08788675189235\n",
      "validation_rmse: 3962.7698842137193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model3 = ModelInterface(raw_data_only_markdown, \"Random forest regression Model with no data transformations. Includes all columns\")\n",
    "model3.model = RandomForestRegressor(n_estimators=100, criterion='mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=True, n_jobs=None, random_state=1, verbose=0, warm_start=False).fit(model3.X_train, model3.y_train)\n",
    "model3.runModel(model3.model)\n",
    "model3.printModel(model3.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: \n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)\n",
      "Description: Linear regression Model with no data transformations. Includes all columns except markdowns\n",
      "train_r2: 0.08833306322917589\n",
      "train_mae: 18.379639828498238\n",
      "train_rmse: 21675.247811126523\n",
      "validation_r2: 0.092406011546733\n",
      "validation_mae: 76.49072870460421\n",
      "validation_rmse: 21674.82239013148\n"
     ]
    }
   ],
   "source": [
    "model4 = ModelInterface(raw_data_no_markdown, \"Linear regression Model with no data transformations. Includes all columns except markdowns\")\n",
    "model4.model = LinearRegression(normalize=True).fit(model4.X_train,model4.y_train)\n",
    "model4.runModel(model4.model)\n",
    "model4.printModel(model4.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: \n",
      "DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "Description: Decision Tree regression Model with no data transformations. Includes all columns except markdowns\n",
      "train_r2: 0.7629119032567171\n",
      "train_mae: 1.827161631005865e-11\n",
      "train_rmse: 11053.526181981952\n",
      "validation_r2: 0.7719734240466052\n",
      "validation_mae: 11.046723206674706\n",
      "validation_rmse: 10864.318152665051\n"
     ]
    }
   ],
   "source": [
    "model5 = ModelInterface(raw_data_no_markdown, \"Decision Tree regression Model with no data transformations. Includes all columns except markdowns\")\n",
    "model5.model = DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n",
    "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=1,\n",
    "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "           presort=False, random_state=None, splitter='best').fit(model5.X_train,model5.y_train)\n",
    "model5.runModel(model5.model)\n",
    "model5.printModel(model5.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: \n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                      n_jobs=None, oob_score=True, random_state=1, verbose=0,\n",
      "                      warm_start=False)\n",
      "Description: Random forest regression Model with no data transformations. Includes all columns except markdowns\n",
      "train_r2: 0.9954672133720925\n",
      "train_mae: 2.3539400974334352\n",
      "train_rmse: 1528.3703612895772\n",
      "validation_r2: 0.9688096528152962\n",
      "validation_mae: 21.575057240790354\n",
      "validation_rmse: 4018.091725050686\n"
     ]
    }
   ],
   "source": [
    "model6 = ModelInterface(raw_data_no_markdown, \"Random forest regression Model with no data transformations. Includes all columns except markdowns\")\n",
    "model6.model = RandomForestRegressor(n_estimators=100, criterion='mse', \n",
    "                                     min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                     max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                     min_impurity_split=None, bootstrap=True, oob_score=True, \n",
    "                                     n_jobs=None, random_state=1, verbose=0, warm_start=False).fit(model6.X_train, model6.y_train)\n",
    "model6.runModel(model6.model)\n",
    "model6.printModel(model6.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: \n",
      "DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "Description: Decision Tree regression Model with no data transformations. Includes all columns including markdowns. This dataset contains data prior to the introduction of markdowns\n",
      "train_r2: 0.09656254543257947\n",
      "train_mae: 9.401427121469734e-12\n",
      "train_rmse: 21577.19641228585\n",
      "validation_r2: 0.09711426429031167\n",
      "validation_mae: 73.92400507043483\n",
      "validation_rmse: 21618.528916645013\n"
     ]
    }
   ],
   "source": [
    "model7 = ModelInterface(raw_data_no_dept, \"Decision Tree regression Model with no data transformations. Includes all columns including markdowns. This dataset contains data prior to the introduction of markdowns and also later\")\n",
    "model7.model = DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n",
    "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=1,\n",
    "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "           presort=False, random_state=None, splitter='best').fit(model7.X_train,model7.y_train)\n",
    "model7.runModel(model7.model)\n",
    "model7.printModel(model7.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: \n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                      n_jobs=None, oob_score=True, random_state=1, verbose=0,\n",
      "                      warm_start=False)\n",
      "Description: Random forest regression Model with no data transformations. This dataset contains data prior to the introduction of markdowns and also later\n",
      "train_r2: 0.10625597624689775\n",
      "train_mae: 1.2867351386963013\n",
      "train_rmse: 21461.127985366205\n",
      "validation_r2: 0.07716769995044315\n",
      "validation_mae: 83.57151518176187\n",
      "validation_rmse: 21856.022824493226\n"
     ]
    }
   ],
   "source": [
    "model8 = ModelInterface(raw_data_no_dept, \"Random forest regression Model with no data transformations. This dataset contains data prior to the introduction of markdowns and also later\")\n",
    "model8.model = RandomForestRegressor(n_estimators=100, criterion='mse', \n",
    "                                     min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                     max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                     min_impurity_split=None, bootstrap=True, oob_score=True, \n",
    "                                     n_jobs=None, random_state=1, verbose=0, warm_start=False).fit(model8.X_train, model8.y_train)\n",
    "model8.runModel(model8.model)\n",
    "model8.printModel(model8.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_no_markdown_week_num = raw_data_no_markdown\n",
    "raw_data_no_markdown['WeekNumber'] = raw_data_no_markdown['Date'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: \n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                      n_jobs=None, oob_score=True, random_state=1, verbose=0,\n",
      "                      warm_start=False)\n",
      "Description: Random forest regression Model with no data transformations. This dataset contains data prior to the introduction of markdowns and also later\n",
      "train_r2: 0.9955032539157406\n",
      "train_mae: 1.9686046922812663\n",
      "train_rmse: 1522.2821385661111\n",
      "validation_r2: 0.96894874494347\n",
      "validation_mae: 21.493581191735636\n",
      "validation_rmse: 4009.1224532362717\n"
     ]
    }
   ],
   "source": [
    "model9 = ModelInterface(raw_data_no_markdown_week_num, \"Random forest regression Model with no data transformations. This dataset contains data prior to the introduction of markdowns and also later\")\n",
    "model9.model = RandomForestRegressor(n_estimators=100, criterion='mse', \n",
    "                                     min_samples_split=2, min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                     max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                     min_impurity_split=None, bootstrap=True, oob_score=True, \n",
    "                                     n_jobs=None, random_state=1, verbose=0, warm_start=False).fit(model9.X_train, model9.y_train)\n",
    "model9.runModel(model9.model)\n",
    "model9.printModel(model9.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_pickle = {\n",
    "    'model1': model1,\n",
    "    'model2': model2,\n",
    "    'model3': model3,\n",
    "    'model4': model4,\n",
    "    'model5': model5,\n",
    "    'model6': model6,\n",
    "    'model7': model7,\n",
    "    'model8': model8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_models.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_models_pickle, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('all_models.pickle', 'rb') as handle:\n",
    "    #all_models_pickle = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_model(model1)\n",
    "store_model(model2)\n",
    "store_model(model3)\n",
    "store_model(model4)\n",
    "store_model(model5)\n",
    "store_model(model6)\n",
    "store_model(model7)\n",
    "store_model(model8)\n",
    "store_model(model9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models=pd.DataFrame(model_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models.to_csv('all_models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = all_models[['Description', 'Model', 'train_r2', 'validation_r2', 'train_mae', \n",
    "       'validation_mae', 'train_rmse', 'validation_rmse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_no_markdown_week_num = raw_data_no_markdown\n",
    "raw_data_no_markdown['WeekNumber'] = raw_data_no_markdown['Date'].dt.week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Stage Final thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "pdpbabi/git/pgpbabi/Capstone/RetailSales.ipynb",
    "public": false
   },
   "id": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "nbpresent": {
   "slides": {
    "badd8c9d-3520-4636-9693-34452231196e": {
     "id": "badd8c9d-3520-4636-9693-34452231196e",
     "layout": "grid",
     "prev": null,
     "regions": {
      "e1e2fe8c-39dd-4441-a14d-e6eb2691b6c9": {
       "attrs": {
        "height": 0.8333333333333334,
        "pad": 0.01,
        "width": 0.8333333333333334,
        "x": 0.08333333333333333,
        "y": 0.08333333333333333
       },
       "id": "e1e2fe8c-39dd-4441-a14d-e6eb2691b6c9"
      }
     }
    }
   },
   "themes": {
    "default": "99602865-96d7-42c6-8876-7a6a3b854ec1",
    "theme": {
     "99602865-96d7-42c6-8876-7a6a3b854ec1": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "99602865-96d7-42c6-8876-7a6a3b854ec1",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "278px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1069px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
